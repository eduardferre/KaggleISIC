{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F_v\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, ConcatDataset\n",
    "from torchvision import models\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73387154",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/new-train-metadata.csv\"\n",
    "TEST_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/students-test-metadata.csv\"\n",
    "TRAIN_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-processed.csv\"\n",
    ")\n",
    "TEST_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/test-metadata-processed.csv\"\n",
    ")\n",
    "TRAIN_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image.hdf5\"\n",
    "TEST_HDF5 = \"/kaggle/input/kaggleisic-challenge/test-image.hdf5\"\n",
    "OUTPUT_FINAL_MODEL = \"/kaggle/working/final_model.pth\"\n",
    "OUTPUT_FINAL_SUBMISSION = \"/kaggle/working/final_submission.csv\"\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"image_type\",\n",
    "    \"patient_id\",\n",
    "    \"copyright_license\",\n",
    "    \"attribution\",\n",
    "    \"anatom_site_general\",\n",
    "    \"tbp_lv_location_simple\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC_HDF5_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that loads images from an HDF5 file given a DataFrame of IDs.\n",
    "    Applies image transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, hdf5_path: str, transform=None, is_labelled: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing 'isic_id' and optionally 'target'.\n",
    "            hdf5_path (str): Path to the HDF5 file containing images.\n",
    "            transform (callable): Optional transforms to be applied on a sample.\n",
    "            is_labelled (bool): Whether the dataset includes labels (for train/val).\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.is_labelled = is_labelled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        isic_id = row[\"isic_id\"]\n",
    "\n",
    "        # Load image from HDF5\n",
    "        image_rgb = self._load_image_from_hdf5(isic_id)\n",
    "\n",
    "        # Apply transforms (PIL-style transforms require converting np array to PIL, or we can do tensor transforms)\n",
    "        if self.transform is not None:\n",
    "            # Convert NumPy array (H x W x C) to a PIL Imag\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = self.transform(image_pil)\n",
    "        else:\n",
    "            # By default, convert it to a PIL Image\n",
    "            view_transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = view_transform(image_pil)\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = row[\"target\"]\n",
    "            label = torch.tensor(label).float()\n",
    "            return image, label, isic_id\n",
    "        else:\n",
    "            return image, isic_id\n",
    "\n",
    "    def _load_image_from_hdf5(self, isic_id: str):\n",
    "        \"\"\"\n",
    "        Loads and decodes an image from HDF5 by isic_id.\n",
    "        Returns a NumPy array in RGB format (H x W x 3).\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hf:\n",
    "            encoded_bytes = hf[isic_id][()]  # uint8 array\n",
    "\n",
    "        # Decode the image bytes with OpenCV (returns BGR)\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        return image_rgb\n",
    "\n",
    "\n",
    "class ISIC_Multimodal_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that loads images from an HDF5 file and metadata from a DataFrame.\n",
    "    Supports optional transforms and training/testing mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, hdf5_path: str, transform=None, is_labelled: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing 'isic_id', metadata features, and optionally 'target'.\n",
    "            hdf5_path (str): Path to the HDF5 file containing images.\n",
    "            transform (callable): Optional image transforms.\n",
    "            is_labelled (bool): Whether the dataset includes labels (for training/validation).\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.is_labelled = is_labelled\n",
    "\n",
    "        # Identify metadata columns (exclude isic_id and target)\n",
    "        self.metadata_cols = [\n",
    "            col for col in self.df.columns if col not in [\"isic_id\", \"target\"]\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        isic_id = row[\"isic_id\"]\n",
    "\n",
    "        # --- Load and transform image ---\n",
    "        image_rgb = self._load_image_from_hdf5(isic_id)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = self.transform(image_pil)\n",
    "        else:\n",
    "            default_transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = default_transform(image_pil)\n",
    "\n",
    "        # --- Load metadata ---\n",
    "        metadata = torch.tensor(row[self.metadata_cols].values.astype(\"float32\"))\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = torch.tensor(row[\"target\"]).float()\n",
    "            return metadata, image, label\n",
    "        else:\n",
    "            return metadata, image, isic_id\n",
    "\n",
    "    def _load_image_from_hdf5(self, isic_id: str):\n",
    "        \"\"\"\n",
    "        Loads and decodes an image from HDF5 by isic_id.\n",
    "        Returns a NumPy array in RGB format (H x W x 3).\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hf:\n",
    "            encoded_bytes = hf[isic_id][()]\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return image_rgb\n",
    "\n",
    "\n",
    "class ISIC_Metadata_Dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, is_labelled: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.is_labelled = is_labelled\n",
    "\n",
    "        # Store input features separately for safety\n",
    "        self.features = df.drop(\n",
    "            columns=[\"target\", \"isic_id\"], errors=\"ignore\"\n",
    "        ).values.astype(\"float32\")\n",
    "\n",
    "        self.isic_ids = df[\"isic_id\"].values.astype(\"str\")\n",
    "\n",
    "        if self.is_labelled:\n",
    "            self.labels = df[\"target\"].values.astype(\"float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        metadata = torch.tensor(self.features[idx])\n",
    "        isic_id = self.isic_ids[idx]\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = torch.tensor(self.labels[idx])\n",
    "            return metadata, label, isic_id\n",
    "        else:\n",
    "            return metadata, isic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_dataset(train_frac=0.8, seed=42) -> tuple:\n",
    "    # Load the metadata CSV files\n",
    "    train_df = pd.read_csv(TRAIN_METADATA_PROCESSED_CSV)\n",
    "    test_df = pd.read_csv(TEST_METADATA_PROCESSED_CSV)\n",
    "\n",
    "    # Perform stratified train/validation split to maintain class distribution\n",
    "    train_dataset, valid_dataset = train_test_split(\n",
    "        train_df, train_size=train_frac, stratify=train_df[\"target\"], random_state=seed\n",
    "    )\n",
    "\n",
    "    # Reset index for train and validation datasets\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    valid_dataset = valid_dataset.reset_index(drop=True)\n",
    "    test_dataset = test_df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"train_dataset shape: {train_dataset.shape}\")\n",
    "    print(f\"valid_dataset shape: {valid_dataset.shape}\")\n",
    "    print(f\"test_dataset shape:  {test_dataset.shape}\")\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def load_hdf5_dataset(\n",
    "    transform: T.Compose, train_frac=0.8, seed=42\n",
    ") -> tuple[ISIC_HDF5_Dataset]:\n",
    "    \"\"\"\n",
    "    Load the ISIC dataset from HDF5 files and split it into train, validation, and test sets.\n",
    "    Args:\n",
    "        transform (T.Compose): Transformations to apply to the images.\n",
    "        train_frac (float): Fraction of the dataset to use for training.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # Load the metadata CSV files\n",
    "    train_df_sub, valid_df_sub, test_df = load_metadata_dataset(\n",
    "        train_frac=train_frac, seed=seed\n",
    "    )\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = ISIC_HDF5_Dataset(\n",
    "        df=train_df_sub, hdf5_path=TRAIN_HDF5, transform=transform, is_labelled=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = ISIC_HDF5_Dataset(\n",
    "        df=valid_df_sub, hdf5_path=TRAIN_HDF5, transform=transform, is_labelled=True\n",
    "    )\n",
    "\n",
    "    test_dataset = ISIC_HDF5_Dataset(\n",
    "        df=test_df, hdf5_path=TEST_HDF5, transform=transform, is_labelled=False\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def load_multimodal_dataset(\n",
    "    transform: T.Compose, train_frac=0.8, seed=42\n",
    ") -> tuple[ISIC_Multimodal_Dataset]:\n",
    "    \"\"\"\n",
    "    Load the ISIC dataset from HDF5 files and split it into train, validation, and test sets.\n",
    "    Args:\n",
    "        transform (T.Compose): Transformations to apply to the images.\n",
    "        train_frac (float): Fraction of the dataset to use for training.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # Load the metadata CSV files\n",
    "    train_df_sub, valid_df_sub, test_df = load_metadata_dataset(\n",
    "        train_frac=train_frac, seed=seed\n",
    "    )\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = ISIC_Multimodal_Dataset(\n",
    "        df=train_df_sub,\n",
    "        hdf5_path=TRAIN_HDF5,\n",
    "        transform=transform,\n",
    "        is_labelled=True,\n",
    "    )\n",
    "\n",
    "    valid_dataset = ISIC_Multimodal_Dataset(\n",
    "        df=valid_df_sub,\n",
    "        hdf5_path=TRAIN_HDF5,\n",
    "        transform=transform,\n",
    "        is_labelled=True,\n",
    "    )\n",
    "\n",
    "    test_dataset = ISIC_Multimodal_Dataset(\n",
    "        df=test_df,\n",
    "        hdf5_path=TEST_HDF5,\n",
    "        transform=transform,\n",
    "        is_labelled=False,\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df, valid_meta_df, test_meta_df = load_metadata_dataset()\n",
    "\n",
    "train_meta_dataset = ISIC_Metadata_Dataset(train_meta_df, is_labelled=True)\n",
    "valid_meta_dataset = ISIC_Metadata_Dataset(valid_meta_df, is_labelled=True)\n",
    "test_meta_dataset = ISIC_Metadata_Dataset(test_meta_df, is_labelled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "train_hdf5_dataset, valid_hdf5_dataset, test_hdf5_dataset = load_hdf5_dataset(\n",
    "    transform=view_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698247",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "train_multi_dataset, valid_multi_dataset, test_multi_dataset = load_multimodal_dataset(\n",
    "    transform=view_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6882d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # batch size\n",
    "NUM_SAMPLES = 5_000  # samples per epoch\n",
    "NUM_WORKERS = min(\n",
    "    4, os.cpu_count() if os.cpu_count() is not None else 2\n",
    ")  # number of CPU threads\n",
    "\n",
    "class_counts = train_meta_df[\"target\"].value_counts().sort_index()\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "sample_weights = train_meta_df[\"target\"].map(class_weights).values\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    replacement=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efbb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "SCHEDULER_STEP_SIZE = 4\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "\n",
    "\n",
    "def train_valid(model, train_loader, valid_loader, is_multimodal=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    # Tracking lists\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        if is_multimodal:\n",
    "            train_acc = train_multimodal(\n",
    "                model, device, train_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "            valid_acc = validate_multimodal(\n",
    "                model, device, valid_loader, criterion, epoch\n",
    "            )\n",
    "        else:\n",
    "            train_acc = train_singles(\n",
    "                model, device, train_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "            valid_acc = validate_singles(model, device, valid_loader, criterion, epoch)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        valid_accuracies.append(valid_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Plot training and validation accuracies\n",
    "    plot_train_valid_curves(train_accuracies, valid_accuracies)\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "\n",
    "def train_eval(\n",
    "    model,\n",
    "    full_loader,\n",
    "    test_loader,\n",
    "    is_multimodal=False,\n",
    "    output_model_file=OUTPUT_FINAL_MODEL,\n",
    "    output_submission_file=OUTPUT_FINAL_SUBMISSION,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    # Tracking lists\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        if is_multimodal:\n",
    "            train_acc = train_multimodal(\n",
    "                model, device, full_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "        else:\n",
    "            train_acc = train_singles(\n",
    "                model, device, full_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save final model\n",
    "    output_model_path = output_model_file\n",
    "    torch.save(model.state_dict(), output_model_path)\n",
    "    print(f\"Model saved to {output_model_path}\")\n",
    "\n",
    "    # Plot training and validation accuracies\n",
    "    plot_train_curves(train_accuracies)\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if is_multimodal:\n",
    "        submission_df = evaluate_multimodal(model, device, test_loader)\n",
    "    else:\n",
    "        submission_df = evaluate_singles(model, device, test_loader)\n",
    "\n",
    "    # Save submission file\n",
    "    submission_file_path = output_submission_file\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Saved submission with {len(submission_df)} rows to {submission_file_path} ✅\"\n",
    "    )\n",
    "\n",
    "\n",
    "def train_multimodal(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for metadatas, images, labels in tqdm(\n",
    "        train_loader, desc=f\"Train Epoch {epoch}\", leave=False\n",
    "    ):\n",
    "        metadatas, images, labels = (\n",
    "            metadatas.to(device).float(),\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, metadatas).view(-1)  # [batch_size]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (logits >= 0.5).float()\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\"\n",
    "    )\n",
    "    return train_accuracy\n",
    "\n",
    "\n",
    "def validate_multimodal(model, device, valid_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for metadatas, images, labels in tqdm(\n",
    "            valid_loader, desc=f\"Validation Epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            metadatas, images, labels = (\n",
    "                metadatas.to(device).float(),\n",
    "                images.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "\n",
    "            logits = model(images, metadatas).view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predicted = (logits >= 0.5).float()\n",
    "            val_correct_preds += (predicted == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    val_accuracy = val_correct_preds / val_total_preds\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\"\n",
    "    )\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "def evaluate_multimodal(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for metadatas, images, isic_ids in tqdm(test_loader, desc=\"Inference on Test\"):\n",
    "            metadatas, images = metadatas.to(device).float(), images.to(device)\n",
    "\n",
    "            logits = model(images, metadatas).view(-1)  # shape [batch_size]\n",
    "            probs = torch.sigmoid(logits)  # shape [batch_size], in [0,1]\n",
    "\n",
    "            probs = probs.cpu().numpy()\n",
    "\n",
    "            for isic_id, p in zip(isic_ids, probs):\n",
    "                predictions.append({\"isic_id\": isic_id, \"target\": float(p)})\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "    submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def train_singles(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for singles, labels, _ in tqdm(\n",
    "        train_loader, desc=f\"Train Epoch {epoch}\", leave=False\n",
    "    ):\n",
    "        singles, labels = (\n",
    "            singles.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(singles).view(-1)  # [batch_size]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (logits >= 0.5).float()\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\"\n",
    "    )\n",
    "    return train_accuracy\n",
    "\n",
    "\n",
    "def validate_singles(model, device, valid_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for singles, labels, _ in tqdm(\n",
    "            valid_loader, desc=f\"Validation Epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            singles, labels = (\n",
    "                singles.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "\n",
    "            logits = model(singles).view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predicted = (logits >= 0.5).float()\n",
    "            val_correct_preds += (predicted == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    val_accuracy = val_correct_preds / val_total_preds\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\"\n",
    "    )\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "def evaluate_singles(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for singles, isic_ids in tqdm(test_loader, desc=\"Inference on Test\"):\n",
    "            singles = singles.to(device)\n",
    "\n",
    "            logits = model(singles).view(-1)  # shape [batch_size]\n",
    "            probs = torch.sigmoid(logits)  # shape [batch_size], in [0,1]\n",
    "\n",
    "            probs = probs.cpu().numpy()\n",
    "\n",
    "            for isic_id, p in zip(isic_ids, probs):\n",
    "                predictions.append({\"isic_id\": isic_id, \"target\": float(p)})\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "    submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def plot_train_valid_curves(train_accs, val_accs):\n",
    "    # Prepare DataFrame for seaborn\n",
    "    epochs = list(range(1, len(train_accs) + 1))\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": epochs * 2,\n",
    "            \"Accuracy\": train_accs + val_accs,\n",
    "            \"Phase\": [\"Train\"] * len(train_accs) + [\"Validation\"] * len(val_accs),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot with seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"Accuracy\", hue=\"Phase\", marker=\"o\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_train_curves(train_accs):\n",
    "    # Prepare DataFrame for seaborn\n",
    "    epochs = list(range(1, len(train_accs) + 1))\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": epochs,\n",
    "            \"Accuracy\": train_accs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot with seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"Accuracy\", marker=\"o\")\n",
    "    plt.title(\"Training Accuracy (Full Dataset)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd91eb",
   "metadata": {},
   "source": [
    "# Only Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_loader = DataLoader(\n",
    "    train_meta_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "valid_meta_loader = DataLoader(\n",
    "    valid_meta_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_meta_loader = DataLoader(\n",
    "    test_meta_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "full_meta_dataset = ConcatDataset([train_meta_dataset, valid_meta_dataset])\n",
    "full_meta_loader = DataLoader(\n",
    "    full_meta_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train loader: {len(train_meta_loader)} batches (total = {NUM_SAMPLES} samples / {BATCH_SIZE} batches)\"\n",
    ")\n",
    "print(f\"Valid loader: {len(valid_meta_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_meta_loader)} batches\")\n",
    "print(f\"Full loader:  {len(full_meta_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b66582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MetadataOnly(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),  # Batch normalization for the hidden layer\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),  # Batch normalization for the second hidden layer\n",
    "            nn.Linear(64, 1),  # Single output unit for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, metadata):\n",
    "        return self.mlp(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea58140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_MetadataOnly(\n",
    "    num_features=train_meta_df.shape[1] - 2\n",
    ")  # Exclude target column\n",
    "\n",
    "train_valid(model, train_meta_loader, valid_meta_loader, is_multimodal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf06466",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(\n",
    "    model,\n",
    "    full_meta_loader,\n",
    "    test_meta_loader,\n",
    "    is_multimodal=False,\n",
    "    output_model_file=\"/kaggle/working/mlp_metadata.pth\",\n",
    "    output_submission_file=\"/kaggle/working/mlp_metadata.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb57c4a",
   "metadata": {},
   "source": [
    "# Only Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92714be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hdf5_loader = DataLoader(\n",
    "    train_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "valid_hdf5_loader = DataLoader(\n",
    "    valid_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_hdf5_loader = DataLoader(\n",
    "    test_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "full_hdf5_dataset = ConcatDataset([train_hdf5_dataset, valid_hdf5_dataset])\n",
    "full_hdf5_loader = DataLoader(\n",
    "    full_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train loader: {len(train_hdf5_loader)} batches (total = {NUM_SAMPLES} samples / {BATCH_SIZE} batches)\"\n",
    ")\n",
    "print(f\"Valid loader: {len(valid_hdf5_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_hdf5_loader)} batches\")\n",
    "print(f\"Full loader:  {len(full_hdf5_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b730c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_ImageOnly(nn.Module):\n",
    "    def __init__(self, image_shape=(3, 224, 224)):\n",
    "        super().__init__()\n",
    "        flattened_size = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = image.view(image.size(0), -1)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ImageOnly()\n",
    "\n",
    "train_valid(model, train_hdf5_loader, valid_hdf5_loader, is_multimodal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830306cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(\n",
    "    model,\n",
    "    full_hdf5_loader,\n",
    "    test_hdf5_loader,\n",
    "    is_multimodal=False,\n",
    "    output_model_file=\"/kaggle/working/mlp_images.pth\",\n",
    "    output_submission_file=\"/kaggle/working/mlp_images.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f622df8",
   "metadata": {},
   "source": [
    "# Multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb83fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_loader = DataLoader(\n",
    "    train_multi_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "valid_multi_loader = DataLoader(\n",
    "    valid_multi_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_multi_loader = DataLoader(\n",
    "    test_multi_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "full_multi_dataset = ConcatDataset([train_multi_dataset, valid_multi_dataset])\n",
    "full_multi_loader = DataLoader(\n",
    "    full_multi_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train loader: {len(train_multi_loader)} batches (total = {NUM_SAMPLES} samples / {BATCH_SIZE} batches)\"\n",
    ")\n",
    "print(f\"Valid loader: {len(valid_multi_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_multi_loader)} batches\")\n",
    "print(f\"Full loader:  {len(full_multi_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Multimodal(nn.Module):\n",
    "    def __init__(self, num_metadata_features, image_shape=(3, 224, 224)):\n",
    "        super().__init__()\n",
    "        flattened_image_size = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        input_size = flattened_image_size + num_metadata_features\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "        image_flat = image.view(image.size(0), -1)\n",
    "        combined = torch.cat((image_flat, metadata), dim=1)\n",
    "        return self.mlp(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79398421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_Multimodal(\n",
    "    num_metadata_features=train_meta_df.shape[1] - 2\n",
    ")  # Exclude target column\n",
    "\n",
    "train_valid(model, train_multi_loader, valid_multi_loader, is_multimodal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(\n",
    "    model,\n",
    "    full_multi_loader,\n",
    "    test_multi_loader,\n",
    "    is_multimodal=True,\n",
    "    output_model_file=\"/kaggle/working/mlp_multimodal.pth\",\n",
    "    output_submission_file=\"/kaggle/working/mlp_multimodal.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
