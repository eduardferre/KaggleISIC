{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70213aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F_v\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import models\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python, NumPy, and Torch seed, ensure deterministic behaviour\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd93e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/new-train-metadata.csv\"\n",
    "TEST_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/students-test-metadata.csv\"\n",
    "TRAIN_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-processed.csv\"\n",
    ")\n",
    "TEST_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/test-metadata-processed.csv\"\n",
    ")\n",
    "TRAIN_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image.hdf5\"\n",
    "TEST_HDF5 = \"/kaggle/input/kaggleisic-challenge/test-image.hdf5\"\n",
    "\n",
    "TRAIN_METADATA_AUGMENTED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-augmented.csv\"\n",
    ")\n",
    "TRAIN_AUGMENTED_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image-augmented.hdf5\"\n",
    "\n",
    "OUTPUT_FINAL_MODEL = \"/kaggle/working/final_model.pth\"\n",
    "OUTPUT_FINAL_SUBMISSION = \"/kaggle/working/final_submission.csv\"\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"image_type\",\n",
    "    \"patient_id\",\n",
    "    \"copyright_license\",\n",
    "    \"attribution\",\n",
    "    \"anatom_site_general\",\n",
    "    \"tbp_lv_location_simple\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC_HDF5_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that loads images from an HDF5 file given a DataFrame of IDs.\n",
    "    Applies image transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, hdf5_path: str, transform=None, is_labelled: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing 'isic_id' and optionally 'target'.\n",
    "            hdf5_path (str): Path to the HDF5 file containing images.\n",
    "            transform (callable): Optional transforms to be applied on a sample.\n",
    "            is_labelled (bool): Whether the dataset includes labels (for train/val).\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.is_labelled = is_labelled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        isic_id = row[\"isic_id\"]\n",
    "\n",
    "        # Load image from HDF5\n",
    "        image_rgb = self._load_image_from_hdf5(isic_id)\n",
    "\n",
    "        # Apply transforms (PIL-style transforms require converting np array to PIL, or we can do tensor transforms)\n",
    "        if self.transform is not None:\n",
    "            # Convert NumPy array (H x W x C) to a PIL Imag\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = self.transform(image_pil)\n",
    "        else:\n",
    "            # By default, convert it to a PIL Image\n",
    "            view_transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = view_transform(image_pil)\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = row[\"target\"]\n",
    "            label = torch.tensor(label).float()\n",
    "            return image, label, isic_id\n",
    "        else:\n",
    "            return image, isic_id\n",
    "\n",
    "    def _load_image_from_hdf5(self, isic_id: str):\n",
    "        \"\"\"\n",
    "        Loads and decodes an image from HDF5 by isic_id.\n",
    "        Returns a NumPy array in RGB format (H x W x 3).\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hf:\n",
    "            encoded_bytes = hf[isic_id][()]  # uint8 array\n",
    "\n",
    "        # Decode the image bytes with OpenCV (returns BGR)\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_dataset_only_healthy(\n",
    "    train_frac=0.8, seed=42, is_subsampled=False, is_augmented=False\n",
    ") -> tuple:\n",
    "    if is_augmented:\n",
    "        train_file = TRAIN_METADATA_AUGMENTED_CSV\n",
    "    else:\n",
    "        train_file = TRAIN_METADATA_PROCESSED_CSV\n",
    "\n",
    "    # Load the metadata CSV files\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(TEST_METADATA_PROCESSED_CSV)\n",
    "\n",
    "    # Filter out healthy cases\n",
    "    train_df = train_df[train_df[\"target\"] == 0].reset_index(drop=True)\n",
    "\n",
    "    # Perform stratified train/validation split to maintain class distribution\n",
    "    train_dataset, valid_dataset = train_test_split(\n",
    "        train_df, train_size=train_frac, stratify=train_df[\"target\"], random_state=seed\n",
    "    )\n",
    "\n",
    "    # Reset index for train and validation datasets\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    valid_dataset = valid_dataset.reset_index(drop=True)\n",
    "    test_dataset = test_df.reset_index(drop=True)\n",
    "\n",
    "    # Optionally create a balanced subset\n",
    "    if is_subsampled:\n",
    "        train_dataset = create_balanced_subset(train_dataset)\n",
    "        valid_dataset = create_balanced_subset(valid_dataset)\n",
    "\n",
    "    print(f\"train_dataset shape: {train_dataset.shape}\")\n",
    "    print(f\"valid_dataset shape: {valid_dataset.shape}\")\n",
    "    print(f\"test_dataset shape:  {test_dataset.shape}\")\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def load_hdf5_dataset(\n",
    "    transform: T.Compose,\n",
    "    train_frac=0.8,\n",
    "    seed=42,\n",
    "    is_subsampled=False,\n",
    "    is_augmented=False,\n",
    ") -> tuple[ISIC_HDF5_Dataset]:\n",
    "    \"\"\"\n",
    "    Load the ISIC dataset from HDF5 files and split it into train, validation, and test sets.\n",
    "    Args:\n",
    "        transform (T.Compose): Transformations to apply to the images.\n",
    "        train_frac (float): Fraction of the dataset to use for training.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # Load the metadata CSV files\n",
    "    train_df_sub, valid_df_sub, test_df = load_metadata_dataset_only_healthy(\n",
    "        train_frac=train_frac,\n",
    "        seed=seed,\n",
    "        is_subsampled=is_subsampled,\n",
    "        is_augmented=is_augmented,\n",
    "    )\n",
    "\n",
    "    if is_augmented:\n",
    "        train_file = TRAIN_AUGMENTED_HDF5\n",
    "    else:\n",
    "        train_file = TRAIN_HDF5\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = ISIC_HDF5_Dataset(\n",
    "        df=train_df_sub, hdf5_path=train_file, transform=transform, is_labelled=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = ISIC_HDF5_Dataset(\n",
    "        df=valid_df_sub, hdf5_path=train_file, transform=transform, is_labelled=True\n",
    "    )\n",
    "\n",
    "    test_dataset = ISIC_HDF5_Dataset(\n",
    "        df=test_df, hdf5_path=TEST_HDF5, transform=transform, is_labelled=False\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def create_balanced_subset(\n",
    "    df: pd.DataFrame, target_col=\"target\", seed=42\n",
    ") -> pd.DataFrame:\n",
    "    # Just keep all the cancer cases and subsample the healthy cases (2:1 ratio)\n",
    "    positives = df[df[target_col] == 1]\n",
    "\n",
    "    n_negatives = len(positives) * 2  # 2:1 ratio\n",
    "    negatives = df[df[target_col] == 0].sample(\n",
    "        n=min(n_negatives, len(df[df[target_col] == 0])), random_state=seed\n",
    "    )\n",
    "    balanced_df = (\n",
    "        pd.concat([positives, negatives])\n",
    "        .sample(frac=1, random_state=seed)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df, valid_meta_df, test_meta_df = load_metadata_dataset_only_healthy(\n",
    "    is_subsampled=True, is_augmented=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "train_hdf5_dataset, valid_hdf5_dataset, test_hdf5_dataset = load_hdf5_dataset(\n",
    "    transform=view_transform, is_subsampled=True, is_augmented=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # batch size\n",
    "NUM_SAMPLES = 1_000  # samples per epoch\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "class_counts = train_meta_df[\"target\"].value_counts().sort_index()\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "sample_weights = train_meta_df[\"target\"].map(class_weights).values\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    replacement=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0415e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "SCHEDULER_STEP_SIZE = 4\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "MIN_DELTA = 0.001\n",
    "\n",
    "\n",
    "def train_valid(model, train_loader, valid_loader, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    # Tracking lists\n",
    "    train_loses = []\n",
    "    valid_loses = []\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_vae(model, device, train_loader, optimizer, epoch)\n",
    "        valid_loss = validate_vae(model, device, valid_loader, epoch)\n",
    "\n",
    "        train_loses.append(train_loss)\n",
    "        valid_loses.append(valid_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Early Stopping check with threshold\n",
    "        if valid_loss > best_valid_loss + MIN_DELTA:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\n",
    "                f\"No improvement in {epochs_no_improve} epochs (threshold of {MIN_DELTA}).\"\n",
    "            )\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs!\")\n",
    "            break\n",
    "\n",
    "    # Plot training and validation ROC AUC scores\n",
    "    plot_train_valid_curves(train_loses, valid_loses)\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train_eval(\n",
    "    model,\n",
    "    full_loader,\n",
    "    test_loader,\n",
    "    early_stopping_epochs=EPOCHS,\n",
    "    output_model_file=\"model_final.pth\",\n",
    "    output_submission_file=\"submission.csv\",\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    # Tracking lists\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(1, early_stopping_epochs + 1):\n",
    "        train_loss = train_vae(model, device, full_loader, optimizer, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save final model\n",
    "    output_model_path = output_model_file\n",
    "    torch.save(model.state_dict(), output_model_path)\n",
    "    print(f\"Model saved to {output_model_path}\")\n",
    "\n",
    "    # Plot training ROC AUC scores\n",
    "    plot_train_curves(train_losses)\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    submission_df = evaluate_vae(model, device, test_loader)\n",
    "\n",
    "    # Save submission file\n",
    "    submission_file_path = output_submission_file\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Saved submission with {len(submission_df)} rows to {submission_file_path} ✅\"\n",
    "    )\n",
    "\n",
    "\n",
    "def train_vae(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for singles, _, _ in tqdm(train_loader, desc=f\"Train Epoch {epoch}\", leave=False):\n",
    "        singles = singles.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_images, mu, logvar = model(singles)\n",
    "\n",
    "        loss = model.loss_function(recon_images, singles, mu, logvar)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f}\")\n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "def validate_vae(model, device, valid_loader, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for singles, _, _ in tqdm(\n",
    "            valid_loader, desc=f\"Validation Epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            singles = singles.to(device)\n",
    "\n",
    "            recon_images, mu, logvar = model(singles)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = model.loss_function(recon_images, singles, mu, logvar)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def evaluate_vae(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for singles, isic_ids in tqdm(test_loader, desc=\"Inference on Test\"):\n",
    "            singles = singles.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_images, mu, logvar = model(singles)\n",
    "\n",
    "            # Calculate reconstruction error (mean squared error for anomaly detection)\n",
    "            reconstruction_error = F.mse_loss(recon_images, singles, reduction=\"none\")\n",
    "            reconstruction_error = reconstruction_error.view(singles.size(0), -1).mean(\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # Threshold for anomaly detection\n",
    "            probability = torch.sigmoid(reconstruction_error).cpu().numpy()\n",
    "            predictions.extend(zip(isic_ids, probability))\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions, columns=[\"isic_id\", \"target\"])\n",
    "    submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def plot_train_valid_curves(train_losses, valid_losses):\n",
    "    # Prepare DataFrame for seaborn\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": epochs * 2,\n",
    "            \"Loss\": train_losses + valid_losses,\n",
    "            \"Phase\": [\"Train\"] * len(train_losses) + [\"Validation\"] * len(valid_losses),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot with seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"Loss\", hue=\"Phase\", marker=\"o\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_train_curves(train_losses):\n",
    "    # Prepare DataFrame for seaborn\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": epochs,\n",
    "            \"Loss\": train_losses,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot with seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"Loss\", marker=\"o\")\n",
    "    plt.title(\"Training Loss (Full Dataset)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f804f",
   "metadata": {},
   "source": [
    "# Only Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4892238",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hdf5_loader = DataLoader(\n",
    "    train_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "valid_hdf5_loader = DataLoader(\n",
    "    valid_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_hdf5_loader = DataLoader(\n",
    "    test_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "full_hdf5_dataset = ConcatDataset([train_hdf5_dataset, valid_hdf5_dataset])\n",
    "full_hdf5_loader = DataLoader(\n",
    "    full_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train loader: {len(train_hdf5_loader)} batches (total = {NUM_SAMPLES} samples / {BATCH_SIZE} batches)\"\n",
    ")\n",
    "print(f\"Valid loader: {len(valid_hdf5_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_hdf5_loader)} batches\")\n",
    "print(f\"Full loader:  {len(full_hdf5_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64 * 28 * 28, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 28 * 28, latent_dim)\n",
    "\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 64 * 28 * 28)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Sigmoid(),  # to ensure output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h1 = self.encoder(x)\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        h2 = self.decoder_fc(z)\n",
    "        x_reconstructed = self.decoder_conv(h2)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = nn.functional.binary_cross_entropy(\n",
    "            recon_x, x.view(-1, 3, 224, 224), reduction=\"sum\"\n",
    "        )\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ca742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "\n",
    "stopping_epochs = train_valid(model, train_hdf5_loader, valid_hdf5_loader, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8822f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = VAE()\n",
    "\n",
    "train_eval(\n",
    "    final_model,\n",
    "    full_hdf5_loader,\n",
    "    test_hdf5_loader,\n",
    "    early_stopping_epochs=stopping_epochs,\n",
    "    output_model_file=\"/kaggle/working/VAE_hdf5.pth\",\n",
    "    output_submission_file=\"/kaggle/working/VAE_hdf5.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
