{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11014d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/new-train-metadata.csv\"\n",
    "TEST_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/students-test-metadata.csv\"\n",
    "TRAIN_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-processed.csv\"\n",
    ")\n",
    "TEST_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/test-metadata-processed.csv\"\n",
    ")\n",
    "TRAIN_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image.hdf5\"\n",
    "TEST_HDF5 = \"/kaggle/input/kaggleisic-challenge/test-image.hdf5\"\n",
    "\n",
    "TRAIN_METADATA_AUGMENTED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-augmented.csv\"\n",
    ")\n",
    "TRAIN_AUGMENTED_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image-augmented.hdf5\"\n",
    "\n",
    "OUTPUT_FINAL_MODEL_LGBM = \"/kaggle/working/lgbm_model.pth\"\n",
    "OUTPUT_FINAL_SUBMISSION_LGBM = \"/kaggle/working/lgbm_submission.csv\"\n",
    "\n",
    "OUTPUT_FINAL_MODEL_XGB = \"/kaggle/working/xgb_model.pth\"\n",
    "OUTPUT_FINAL_SUBMISSION_XGB = \"/kaggle/working/xgb_submission.csv\"\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"image_type\",\n",
    "    \"patient_id\",\n",
    "    \"copyright_license\",\n",
    "    \"attribution\",\n",
    "    \"anatom_site_general\",\n",
    "    \"tbp_lv_location_simple\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_dataset(\n",
    "    train_frac=0.8, seed=42, is_subsampled=False, is_augmented=False\n",
    ") -> tuple:\n",
    "    if is_augmented:\n",
    "        train_file = TRAIN_METADATA_AUGMENTED_CSV\n",
    "    else:\n",
    "        train_file = TRAIN_METADATA_PROCESSED_CSV\n",
    "\n",
    "    # Load the metadata CSV files\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(TEST_METADATA_PROCESSED_CSV)\n",
    "\n",
    "    # Perform stratified train/validation split to maintain class distribution\n",
    "    train_dataset, valid_dataset = train_test_split(\n",
    "        train_df, train_size=train_frac, stratify=train_df[\"target\"], random_state=seed\n",
    "    )\n",
    "\n",
    "    # Reset index for train and validation datasets\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    valid_dataset = valid_dataset.reset_index(drop=True)\n",
    "    test_dataset = test_df.reset_index(drop=True)\n",
    "\n",
    "    # Optionally create a balanced subset\n",
    "    if is_subsampled:\n",
    "        train_dataset = create_balanced_subset(train_dataset)\n",
    "        valid_dataset = create_balanced_subset(valid_dataset)\n",
    "\n",
    "    print(f\"train_dataset shape: {train_dataset.shape}\")\n",
    "    print(f\"valid_dataset shape: {valid_dataset.shape}\")\n",
    "    print(f\"test_dataset shape:  {test_dataset.shape}\")\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def create_balanced_subset(\n",
    "    df: pd.DataFrame, target_col=\"target\", seed=42\n",
    ") -> pd.DataFrame:\n",
    "    # Just keep all the cancer cases and subsample the healthy cases (2:1 ratio)\n",
    "    positives = df[df[target_col] == 1]\n",
    "\n",
    "    n_negatives = len(positives) * 2  # 2:1 ratio\n",
    "    negatives = df[df[target_col] == 0].sample(\n",
    "        n=min(n_negatives, len(df[df[target_col] == 0])), random_state=seed\n",
    "    )\n",
    "    balanced_df = (\n",
    "        pd.concat([positives, negatives])\n",
    "        .sample(frac=1, random_state=seed)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df, valid_meta_df, test_meta_df = load_metadata_dataset(\n",
    "    is_subsampled=True, is_augmented=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "AUC_scorer = make_scorer(roc_auc_score)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "inner_AUC = {}\n",
    "outer_results = {}\n",
    "\n",
    "full_meta_df = pd.concat([train_meta_df, valid_meta_df], axis=0)\n",
    "X_train = full_meta_df.drop(columns=[\"target\"])\n",
    "Y_train = full_meta_df[\"target\"]\n",
    "X_test = test_meta_df.drop(columns=[\"target\"])\n",
    "Y_test = test_meta_df[\"target\"]\n",
    "\n",
    "\n",
    "def lgbm_objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 750),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        \"verbosity\": -1,\n",
    "        \"class_weight\": \"balanced\",\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "\n",
    "    return cross_val_score(model, X_train, Y_train, cv=inner, scoring=\"roc_auc\").mean()\n",
    "\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "lgbm_study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "lgbm_study.optimize(lgbm_objective, n_trials=30)\n",
    "\n",
    "best_params_lgbm = lgbm_study.best_params\n",
    "best_params_lgbm[\"random_state\"] = RANDOM_SEED\n",
    "best_params_lgbm[\"verbosity\"] = -1\n",
    "best_params_lgbm[\"class_weight\"] = \"balanced\"\n",
    "\n",
    "print(\"Best params:\", best_params_lgbm)\n",
    "\n",
    "lgbm_model = LGBMClassifier(**best_params_lgbm)\n",
    "outer_auc = cross_val_score(\n",
    "    lgbm_model, X_train, Y_train, cv=outer, scoring=\"roc_auc\"\n",
    ").mean()\n",
    "\n",
    "inner_AUC[\"LightGBM\"] = lgbm_study.best_value\n",
    "outer_results[\"LightGBM\"] = outer_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "AUC_scorer = make_scorer(roc_auc_score)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "inner_AUC = {}\n",
    "outer_results = {}\n",
    "\n",
    "full_meta_df = pd.concat([train_meta_df, valid_meta_df], axis=0)\n",
    "X_train = full_meta_df.drop(columns=[\"target\"])\n",
    "Y_train = full_meta_df[\"target\"]\n",
    "X_test = test_meta_df.drop(columns=[\"target\"])\n",
    "Y_test = test_meta_df[\"target\"]\n",
    "\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 750),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "\n",
    "    return cross_val_score(model, X_train, Y_train, cv=inner, scoring=\"roc_auc\").mean()\n",
    "\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "xgb_study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "xgb_study.optimize(xgb_objective, n_trials=30)\n",
    "\n",
    "best_params_xgb = xgb_study.best_params\n",
    "best_params_xgb[\"random_state\"] = RANDOM_SEED\n",
    "\n",
    "print(\"Best params:\", best_params_xgb)\n",
    "\n",
    "xgb_model = XGBClassifier(**best_params_xgb)\n",
    "outer_auc = cross_val_score(\n",
    "    xgb_model, X_train, Y_train, cv=outer, scoring=\"roc_auc\"\n",
    ").mean()\n",
    "\n",
    "inner_AUC[\"XGBoost\"] = xgb_study.best_value\n",
    "outer_results[\"XGBoost\"] = outer_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inner AUC scores:\", inner_AUC)\n",
    "print(\"Outer AUC scores:\", outer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78374595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar los modelos en todo el conjunto de entrenamiento\n",
    "lgbm_model.fit(X_train, Y_train)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de test\n",
    "lgbm_preds = lgbm_model.predict(X_test)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Calcular Accuracy y AUC en test\n",
    "lgbm_test_accuracy = accuracy_score(Y_test, lgbm_preds)\n",
    "lgbm_test_auc = roc_auc_score(Y_test, lgbm_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "xgb_test_accuracy = accuracy_score(Y_test, xgb_preds)\n",
    "xgb_test_auc = roc_auc_score(Y_test, xgb_model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LightGBM Test Accuracy:\", lgbm_test_accuracy)\n",
    "print(\"LightGBM Test AUC:\", lgbm_test_auc)\n",
    "print(\"XGBoost Test Accuracy:\", xgb_test_accuracy)\n",
    "print(\"XGBoost Test AUC:\", xgb_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d393f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "results_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [model for model in inner_AUC.keys()],\n",
    "        \"AUC\": [inner_AUC[model] for model in inner_AUC.keys()],\n",
    "    }\n",
    ")\n",
    "\n",
    "results_table = results_table.style.set_properties(\n",
    "    **{\"text-align\": \"center\", \"border\": \"1px solid black\", \"background-color\": \"white\"}\n",
    ").highlight_max(subset=[\"AUC\"], axis=0, color=\"lightgreen\")\n",
    "\n",
    "# Display both tables\n",
    "display(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PREDICCIÓN FINAL CON LGBM ----\n",
    "# 1. Preparar datos\n",
    "train_full = full_meta_df.drop(columns=[\"target\"]).copy()\n",
    "test_full = test_meta_df.drop(columns=[\"target\"]).copy()\n",
    "\n",
    "# Asegurar orden de columnas\n",
    "test_full = test_full[train_full.columns]\n",
    "\n",
    "# 2. Codificar target\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_full = label_encoder.fit_transform(full_meta_df[\"target\"])\n",
    "\n",
    "# 3. Instanciar modelo con mejores hiperparámetros\n",
    "lgbm_final = LGBMClassifier(**best_params_lgbm)\n",
    "\n",
    "# 4. Entrenar en todos los datos\n",
    "lgbm_final.fit(train_full, y_train_full)\n",
    "\n",
    "# 5. Predecir probabilidades sobre el conjunto de test\n",
    "lgbm_test_probs = lgbm_final.predict_proba(test_full)[:, 1]\n",
    "\n",
    "# 6. Guardar en CSV\n",
    "lgbm_submission = pd.DataFrame(\n",
    "    {\"isic_id\": test_meta_df[\"isic_id\"], \"lgbm_proba\": lgbm_test_probs}\n",
    ")\n",
    "\n",
    "lgbm_submission.rename(columns={\"lgbm_proba\": \"target\"}, inplace=True)\n",
    "lgbm_submission.to_csv(OUTPUT_FINAL_SUBMISSION_LGBM, index=False)\n",
    "\n",
    "print(\"✅ Predicciones LGBM guardadas en 'lgbm_predictions.csv'\")\n",
    "display(lgbm_submission.head())\n",
    "\n",
    "# 7. Guardar modelo\n",
    "dump(lgbm_final, OUTPUT_FINAL_MODEL_LGBM)\n",
    "print(f\"✅ Modelo LGBM guardado en '{OUTPUT_FINAL_MODEL_LGBM}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c67354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PREDICCIÓN FINAL CON XGBOOST ----\n",
    "# 1. Preparar datos (ya definidos antes)\n",
    "# train_full, test_full, label_encoder, y_train_full\n",
    "\n",
    "# 2. Instanciar modelo con mejores hiperparámetros\n",
    "xgb_final = XGBClassifier(**best_params_xgb)\n",
    "\n",
    "# 3. Entrenar en todos los datos\n",
    "xgb_final.fit(train_full, y_train_full)\n",
    "\n",
    "# 4. Predecir probabilidades sobre el conjunto de test\n",
    "xgb_test_probs = xgb_final.predict_proba(test_full)[:, 1]\n",
    "\n",
    "# 5. Guardar en CSV\n",
    "xgb_submission = pd.DataFrame(\n",
    "    {\"isic_id\": test_meta_df[\"isic_id\"], \"xgb_proba\": xgb_test_probs}\n",
    ")\n",
    "\n",
    "xgb_submission.rename(columns={\"xgb_proba\": \"target\"}, inplace=True)\n",
    "xgb_submission.to_csv(OUTPUT_FINAL_SUBMISSION_XGB, index=False)\n",
    "\n",
    "print(\"✅ Predicciones XGBoost guardadas en 'xgb_predictions.csv'\")\n",
    "display(xgb_submission.head())\n",
    "\n",
    "# 6. Guardar modelo\n",
    "dump(xgb_final, OUTPUT_FINAL_MODEL_XGB)\n",
    "print(f\"✅ Modelo XGBoost guardado en '{OUTPUT_FINAL_MODEL_XGB}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
