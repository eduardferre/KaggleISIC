{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb929fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F_v\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, ConcatDataset\n",
    "from torchvision import models\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python, NumPy, and Torch seed, ensure deterministic behaviour\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f55543",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/new-train-metadata.csv\"\n",
    "TEST_METADATA_CSV = \"/kaggle/input/kaggleisic-challenge/students-test-metadata.csv\"\n",
    "TRAIN_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-processed.csv\"\n",
    ")\n",
    "TEST_METADATA_PROCESSED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/test-metadata-processed.csv\"\n",
    ")\n",
    "TRAIN_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image.hdf5\"\n",
    "TEST_HDF5 = \"/kaggle/input/kaggleisic-challenge/test-image.hdf5\"\n",
    "\n",
    "TRAIN_METADATA_AUGMENTED_CSV = (\n",
    "    \"/kaggle/input/kaggleisic-challenge/train-metadata-augmented.csv\"\n",
    ")\n",
    "TRAIN_AUGMENTED_HDF5 = \"/kaggle/input/kaggleisic-challenge/train-image-augmented.hdf5\"\n",
    "\n",
    "OUTPUT_FINAL_MODEL = \"/kaggle/working/final_model.pth\"\n",
    "OUTPUT_FINAL_SUBMISSION = \"/kaggle/working/final_submission.csv\"\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"image_type\",\n",
    "    \"patient_id\",\n",
    "    \"copyright_license\",\n",
    "    \"attribution\",\n",
    "    \"anatom_site_general\",\n",
    "    \"tbp_lv_location_simple\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC_HDF5_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that loads images from an HDF5 file given a DataFrame of IDs.\n",
    "    Applies image transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, hdf5_path: str, transform=None, is_labelled: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing 'isic_id' and optionally 'target'.\n",
    "            hdf5_path (str): Path to the HDF5 file containing images.\n",
    "            transform (callable): Optional transforms to be applied on a sample.\n",
    "            is_labelled (bool): Whether the dataset includes labels (for train/val).\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.is_labelled = is_labelled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        isic_id = row[\"isic_id\"]\n",
    "\n",
    "        # Load image from HDF5\n",
    "        image_rgb = self._load_image_from_hdf5(isic_id)\n",
    "\n",
    "        # Apply transforms (PIL-style transforms require converting np array to PIL, or we can do tensor transforms)\n",
    "        if self.transform is not None:\n",
    "            # Convert NumPy array (H x W x C) to a PIL Imag\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = self.transform(image_pil)\n",
    "        else:\n",
    "            # By default, convert it to a PIL Image\n",
    "            view_transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = view_transform(image_pil)\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = row[\"target\"]\n",
    "            label = torch.tensor(label).float()\n",
    "            return image, label, isic_id\n",
    "        else:\n",
    "            return image, isic_id\n",
    "\n",
    "    def _load_image_from_hdf5(self, isic_id: str):\n",
    "        \"\"\"\n",
    "        Loads and decodes an image from HDF5 by isic_id.\n",
    "        Returns a NumPy array in RGB format (H x W x 3).\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hf:\n",
    "            encoded_bytes = hf[isic_id][()]  # uint8 array\n",
    "\n",
    "        # Decode the image bytes with OpenCV (returns BGR)\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_dataset(\n",
    "    train_frac=0.8, seed=42, is_subsampled=False, is_augmented=False\n",
    ") -> tuple:\n",
    "    if is_augmented:\n",
    "        train_file = TRAIN_METADATA_AUGMENTED_CSV\n",
    "    else:\n",
    "        train_file = TRAIN_METADATA_PROCESSED_CSV\n",
    "\n",
    "    # Load the metadata CSV files\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(TEST_METADATA_PROCESSED_CSV)\n",
    "\n",
    "    # Perform stratified train/validation split to maintain class distribution\n",
    "    train_dataset, valid_dataset = train_test_split(\n",
    "        train_df, train_size=train_frac, stratify=train_df[\"target\"], random_state=seed\n",
    "    )\n",
    "\n",
    "    # Reset index for train and validation datasets\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    valid_dataset = valid_dataset.reset_index(drop=True)\n",
    "    test_dataset = test_df.reset_index(drop=True)\n",
    "\n",
    "    # Optionally create a balanced subset\n",
    "    if is_subsampled:\n",
    "        train_dataset = create_balanced_subset(train_dataset)\n",
    "        valid_dataset = create_balanced_subset(valid_dataset)\n",
    "\n",
    "    print(f\"train_dataset shape: {train_dataset.shape}\")\n",
    "    print(f\"valid_dataset shape: {valid_dataset.shape}\")\n",
    "    print(f\"test_dataset shape:  {test_dataset.shape}\")\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def load_hdf5_dataset(\n",
    "    transform: T.Compose,\n",
    "    train_frac=0.8,\n",
    "    seed=42,\n",
    "    is_subsampled=False,\n",
    "    is_augmented=False,\n",
    ") -> tuple[ISIC_HDF5_Dataset]:\n",
    "    \"\"\"\n",
    "    Load the ISIC dataset from HDF5 files and split it into train, validation, and test sets.\n",
    "    Args:\n",
    "        transform (T.Compose): Transformations to apply to the images.\n",
    "        train_frac (float): Fraction of the dataset to use for training.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # Load the metadata CSV files\n",
    "    train_df_sub, valid_df_sub, test_df = load_metadata_dataset(\n",
    "        train_frac=train_frac,\n",
    "        seed=seed,\n",
    "        is_subsampled=is_subsampled,\n",
    "        is_augmented=is_augmented,\n",
    "    )\n",
    "\n",
    "    if is_augmented:\n",
    "        train_file = TRAIN_AUGMENTED_HDF5\n",
    "    else:\n",
    "        train_file = TRAIN_HDF5\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = ISIC_HDF5_Dataset(\n",
    "        df=train_df_sub, hdf5_path=train_file, transform=transform, is_labelled=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = ISIC_HDF5_Dataset(\n",
    "        df=valid_df_sub, hdf5_path=train_file, transform=transform, is_labelled=True\n",
    "    )\n",
    "\n",
    "    test_dataset = ISIC_HDF5_Dataset(\n",
    "        df=test_df, hdf5_path=TEST_HDF5, transform=transform, is_labelled=False\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def create_balanced_subset(\n",
    "    df: pd.DataFrame, target_col=\"target\", seed=42\n",
    ") -> pd.DataFrame:\n",
    "    # Just keep all the cancer cases and subsample the healthy cases (2:1 ratio)\n",
    "    positives = df[df[target_col] == 1]\n",
    "\n",
    "    n_negatives = len(positives) * 2  # 2:1 ratio\n",
    "    negatives = df[df[target_col] == 0].sample(\n",
    "        n=min(n_negatives, len(df[df[target_col] == 0])), random_state=seed\n",
    "    )\n",
    "    balanced_df = (\n",
    "        pd.concat([positives, negatives])\n",
    "        .sample(frac=1, random_state=seed)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df, valid_meta_df, test_meta_df = load_metadata_dataset(\n",
    "    is_subsampled=True, is_augmented=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1cff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "train_hdf5_dataset, valid_hdf5_dataset, test_hdf5_dataset = load_hdf5_dataset(\n",
    "    transform=view_transform, is_subsampled=True, is_augmented=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780864",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # batch size\n",
    "NUM_SAMPLES = 500  # samples per epoch\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "class_counts = train_meta_df[\"target\"].value_counts().sort_index()\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "sample_weights = train_meta_df[\"target\"].map(class_weights).values\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    replacement=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "SCHEDULER_STEP_SIZE = 4\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "MIN_DELTA = 0.001\n",
    "\n",
    "\n",
    "def train_valid(model, train_loader, valid_loader, patience=5, is_multimodal=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    # Tracking lists\n",
    "    train_aucs = []\n",
    "    valid_aucs = []\n",
    "\n",
    "    best_valid_auc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        if is_multimodal:\n",
    "            train_auc = train_multimodal(\n",
    "                model, device, train_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "            valid_auc = validate_multimodal(\n",
    "                model, device, valid_loader, criterion, epoch\n",
    "            )\n",
    "        else:\n",
    "            train_auc = train_singles(\n",
    "                model, device, train_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "            valid_auc = validate_singles(model, device, valid_loader, criterion, epoch)\n",
    "\n",
    "        train_aucs.append(train_auc)\n",
    "        valid_aucs.append(valid_auc)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Early Stopping check with threshold\n",
    "        if valid_auc > best_valid_auc + MIN_DELTA:\n",
    "            best_valid_auc = valid_auc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\n",
    "                f\"No improvement in {epochs_no_improve} epochs (threshold of {MIN_DELTA}).\"\n",
    "            )\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs!\")\n",
    "            break\n",
    "\n",
    "    # Plot training and validation ROC AUC scores\n",
    "    plot_train_valid_curves(train_aucs, valid_aucs)\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train_eval(\n",
    "    model,\n",
    "    full_loader,\n",
    "    test_loader,\n",
    "    early_stopping_epochs=EPOCHS,\n",
    "    is_multimodal=False,\n",
    "    output_model_file=\"model_final.pth\",\n",
    "    output_submission_file=\"submission.csv\",\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    # Tracking lists\n",
    "    train_aucs = []\n",
    "\n",
    "    for epoch in range(1, early_stopping_epochs + 1):\n",
    "        if is_multimodal:\n",
    "            train_auc = train_multimodal(\n",
    "                model, device, full_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "        else:\n",
    "            train_auc = train_singles(\n",
    "                model, device, full_loader, optimizer, criterion, epoch\n",
    "            )\n",
    "        train_aucs.append(train_auc)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save final model\n",
    "    output_model_path = output_model_file\n",
    "    torch.save(model.state_dict(), output_model_path)\n",
    "    print(f\"Model saved to {output_model_path}\")\n",
    "\n",
    "    # Plot training ROC AUC scores\n",
    "    plot_train_curves(train_aucs)\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if is_multimodal:\n",
    "        submission_df = evaluate_multimodal(model, device, test_loader)\n",
    "    else:\n",
    "        submission_df = evaluate_singles(model, device, test_loader)\n",
    "\n",
    "    # Save submission file\n",
    "    submission_file_path = output_submission_file\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Saved submission with {len(submission_df)} rows to {submission_file_path} ✅\"\n",
    "    )\n",
    "\n",
    "\n",
    "def train_singles(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for singles, labels, _ in tqdm(\n",
    "        train_loader, desc=f\"Train Epoch {epoch}\", leave=False\n",
    "    ):\n",
    "        singles, labels = singles.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(singles).view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_logits.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    try:\n",
    "        train_auc = roc_auc_score(all_labels, all_logits)\n",
    "    except ValueError:\n",
    "        train_auc = 0.0  # In case only one class present\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Train ROC AUC: {train_auc:.4f}\"\n",
    "    )\n",
    "    return train_auc\n",
    "\n",
    "\n",
    "def validate_singles(model, device, valid_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for singles, labels, _ in tqdm(\n",
    "            valid_loader, desc=f\"Validation Epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            singles, labels = singles.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(singles).view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            all_logits.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    try:\n",
    "        val_auc = roc_auc_score(all_labels, all_logits)\n",
    "    except ValueError:\n",
    "        val_auc = 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Validation Loss: {avg_val_loss:.4f} | Validation ROC AUC: {val_auc:.4f}\"\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "def evaluate_singles(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for singles, isic_ids in tqdm(test_loader, desc=\"Inference on Test\"):\n",
    "            singles = singles.to(device)\n",
    "\n",
    "            logits = model(singles).view(-1)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for isic_id, p in zip(isic_ids, probs):\n",
    "                predictions.append({\"isic_id\": isic_id, \"target\": float(p)})\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "    submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def train_multimodal(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for metadatas, images, labels in tqdm(\n",
    "        train_loader, desc=f\"Train Epoch {epoch}\", leave=False\n",
    "    ):\n",
    "        metadatas, images, labels = (\n",
    "            metadatas.to(device).float(),\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, metadatas).view(-1)  # [batch_size]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_logits.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    try:\n",
    "        train_auc = roc_auc_score(all_labels, all_logits)\n",
    "    except ValueError:\n",
    "        train_auc = 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Train ROC AUC: {train_auc:.4f}\"\n",
    "    )\n",
    "    return train_auc\n",
    "\n",
    "\n",
    "def validate_multimodal(model, device, valid_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for metadatas, images, labels in tqdm(\n",
    "            valid_loader, desc=f\"Validation Epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            metadatas, images, labels = (\n",
    "                metadatas.to(device).float(),\n",
    "                images.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "\n",
    "            logits = model(images, metadatas).view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            all_logits.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    try:\n",
    "        val_auc = roc_auc_score(all_labels, all_logits)\n",
    "    except ValueError:\n",
    "        val_auc = 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Validation Loss: {avg_val_loss:.4f} | Validation ROC AUC: {val_auc:.4f}\"\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "def evaluate_multimodal(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for metadatas, images, isic_ids in tqdm(test_loader, desc=\"Inference on Test\"):\n",
    "            metadatas, images = metadatas.to(device).float(), images.to(device)\n",
    "\n",
    "            logits = model(images, metadatas).view(-1)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for isic_id, p in zip(isic_ids, probs):\n",
    "                predictions.append({\"isic_id\": isic_id, \"target\": float(p)})\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "    submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def plot_train_valid_curves(train_aucs, val_aucs):\n",
    "    # Prepare DataFrame for seaborn\n",
    "    epochs = list(range(1, len(train_aucs) + 1))\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": epochs * 2,\n",
    "            \"ROC AUC\": train_aucs + val_aucs,\n",
    "            \"Phase\": [\"Train\"] * len(train_aucs) + [\"Validation\"] * len(val_aucs),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot with seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"ROC AUC\", hue=\"Phase\", marker=\"o\")\n",
    "    plt.title(\"Training vs Validation ROC AUC\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_train_curves(train_aucs):\n",
    "    # Prepare DataFrame for seaborn\n",
    "    epochs = list(range(1, len(train_aucs) + 1))\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": epochs,\n",
    "            \"ROC AUC\": train_aucs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot with seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"ROC AUC\", marker=\"o\")\n",
    "    plt.title(\"Training ROC AUC (Full Dataset)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c39d96",
   "metadata": {},
   "source": [
    "# Only Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hdf5_loader = DataLoader(\n",
    "    train_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "valid_hdf5_loader = DataLoader(\n",
    "    valid_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_hdf5_loader = DataLoader(\n",
    "    test_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "full_hdf5_dataset = ConcatDataset([train_hdf5_dataset, valid_hdf5_dataset])\n",
    "full_hdf5_loader = DataLoader(\n",
    "    full_hdf5_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train loader: {len(train_hdf5_loader)} batches (total = {NUM_SAMPLES} samples / {BATCH_SIZE} batches)\"\n",
    ")\n",
    "print(f\"Valid loader: {len(valid_hdf5_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_hdf5_loader)} batches\")\n",
    "print(f\"Full loader:  {len(full_hdf5_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_Simple_ImageOnly(nn.Module):\n",
    "    def __init__(self, out_features=1):\n",
    "        super(ResNet50_Simple_ImageOnly, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de130201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50_Simple_ImageOnly()\n",
    "\n",
    "stopping_epochs = train_valid(\n",
    "    model, train_hdf5_loader, valid_hdf5_loader, patience=5, is_multimodal=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db974539",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = ResNet50_Simple_ImageOnly()\n",
    "\n",
    "train_eval(\n",
    "    final_model,\n",
    "    full_hdf5_loader,\n",
    "    test_hdf5_loader,\n",
    "    early_stopping_epochs=stopping_epochs,\n",
    "    is_multimodal=False,\n",
    "    output_model_file=\"/kaggle/working/bench_hdf5.pth\",\n",
    "    output_submission_file=\"/kaggle/working/bench_hdf5.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
